{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 1\n",
    "### Code\n",
    "\n",
    "Variable names are confusing: x1 and x2 - since the convention is to use x for the predictor variables and y for the feature variable\n",
    "d is also a confusing variable name - at least call it 'data' but better to do something like 'salary data'\n",
    "\n",
    "### Methodology / Conceptual understanding\n",
    "\n",
    "x2 (ContractType) is used to predict x1 (Salary). Consider including other columns from the data to use as predictors - Company, ContractType, etc. - as well as potentially the interactions between the predictors.\n",
    "\n",
    "Since cross_valscore was calle with cv=1, only one cross validation fold was used. Cross-validation should divide your data into multiple training and test splits, and train the model using the training split and then testing the model using the test split. By selecting cv=5, for example, you'd subdivide the data 5 times, and use four fifths of the data to train the model, and the remaining fifth to test the model. You'd then repeat the process five more times, each time using a different fifth of the data to test. This process is used to ensure that the model has not been overfit. If there is evidence of overfitting (based on differences between train and test) then the hyperparameter should be adjusted to reduce the overfitting. One approach is to use cross validation results to choose the right parameters with a grid search.\n",
    "There is extensive documentation on this here: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation. If the student was having trouble, this would definately be a topic to discuss, rather than to just give some typed comments on the assignment - a good understanding of how to avoid overfitting is crucial.\n",
    "\n",
    "Model is scored using mean_absolute_error. Consider what error metric your stakeholder would like... perhaps something like, \"we can explain 30% of the variation in Salary using ContractType.\" Your stakeholder may not understand SalaryNormalized, so you may want to report the error in terms of Salary rather than SalaryNormalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 2\n",
    "\n",
    "### Code\n",
    "\n",
    "Use a more descriptive variable name than \"data\"\n",
    "\n",
    "### Methodology / Conceptual Understanding\n",
    "\n",
    "ContractType is the only variable used to predict SalaryNormalized. Consider whether other variables, such as Company, Category, LocationNormalized could be used. You wouldn't want to use Salary to predict SalaryNormalized since SalaryNormalized is a transformed version of Salary.\n",
    "\n",
    "Results: You output the average mean absolute error of the cross validation, but your stakeholder might also be interested to how SalaryNormalized (or maybe Salary) differ by Contract Type. You could do a boxplot to show this, or output the coeifficients from the regression.\n",
    "\n",
    "Model is scored using mean_absolute_error. Consider what error metric your stakeholder would like... perhaps something like, \"we can explain 30% of the variation in Salary using ContractType.\" Your stakeholder may not understand SalaryNormalized, so you may want to report the error in terms of Salary rather than SalaryNormalized. \"r2_score\" will give you the percent of varation in SalaryNormalized explained by the model.\n",
    "\n",
    "Consider what else your stakeholder would like to know about the model - would the average salary (represented by the intercept)? You might want to explain this model by saying that there is an overall average salary, and ContractType will predict an increase or decrease. Box plots showing salary by contract type could be helpful.\n",
    "Cross validation with cv=5 will create 5 models, each of which will use 4/5ths of the data to train the model and the remaining 1/5 to test. So, here you are creating five different models. Which model will you use to make predictions in the future? Cross-validation is used to choose optimal hyperparameters. Once you have an optimal hyperparameter, you should run it on the full training data (no folds) to produce a final model. However, the linear regression method you're using doesn't have a hyperparamer to tune. You could instead use sklearn.linear_model.Ridge or sklearn.linear_model.ElasticNet. Use GridSearchCV to select the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
